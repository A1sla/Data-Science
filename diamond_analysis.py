# -*- coding: utf-8 -*-
"""Diamond Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10qjfeNal-JALuZNi1aqVsDIbUQmmOK9g
"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('/content/drive/MyDrive/Python, Data Mining, ETC/Datasets/diamonds.csv')
df

df['clarity'].mode()

df['cut'].mode()

df['price'].mode()

df['carat'].mode()

df['carat'].mean()

df.describe()

df.info()

plt.figure(figsize = (10, 5))
sns.histplot(df['price'], bins = 20, kde=True)

plt.figure(figsize = (10, 5))
sns.histplot(df['carat'], bins=20, kde=True)

plt.pie(df['cut'].value_counts(),labels=['Ideal','Premium','Very Good','Good','Fair'],autopct='%1.1f%%')
plt.title('Cut')
plt.show()

plt.figure(figsize = (12, 5))
sns.barplot(x='cut',
            y='price',
            data = df)

col_list = df.columns.tolist()
col_list.remove('cut')

for col in col_list:
  plt.title(col)
  sns.histplot(x=col, data=df, hue='cut')
  plt.show()

"""## Removing outliers"""

df = df[(df["x"]<30)]
df = df[(df["y"]<30)]
df = df[(df["z"]<30)&(df["z"]>2)]
df = df[(df["depth"]<75)&(df["depth"]>45)]
df = df[(df["table"]<80)&(df["table"]>40)]

df.shape

sns.pairplot(data=df, hue='cut', palette='Paired')
plt.show()

# List of categorical
# Get list of categorical variables
s = (df.dtypes =="object")
object_cols = list(s[s].index)
print("Categorical variables:")
print(object_cols)

corr_matrix = df.corr()

# Creating a heatmap using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

plt.figure(figsize = (12, 5))
sns.lineplot(x='carat',
             y='price',
             data = df)
plt.title('Carat vs Price')
plt.show()

plt.figure(figsize = (12, 5))
sns.lineplot(x='x',
             y='price',
             data = df)
plt.title('x vs Price')
plt.show()

plt.figure(figsize = (12, 5))
sns.lineplot(x='y',
             y='price',
             data = df)
plt.title('y vs Price')
plt.show()

plt.figure(figsize = (12, 5))
sns.lineplot(x='z',
             y='price',
             data = df)
plt.title('z vs Price')
plt.show()

"""# Label Encoding for categorical data

"""

from sklearn.preprocessing import OneHotEncoder, LabelEncoder #Package for encoding

# make copy of data
df_labeled = df.copy()
le = LabelEncoder()
for col in object_cols:
  df_labeled[col] = le.fit_transform(df_labeled[col])
df_labeled.head()
# Use no-loop method to transform

df.describe()

"""# Modelling

"""

# Import library for modelling
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor

from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn import metrics

X = df_labeled.drop(["price"],axis =1)
y = df_labeled["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)

X_train.shape

X_test.shape

print("\nFirst few rows of Train Data:")
print("X_train:")
print(X_train[:5])  # First 5 rows of X_train
print("\ny_train:")
print(y_train[:5])  # First 5 rows of y_train

print("\nFirst few rows of Test Data:")
print("X_test:")
print(X_test[:5])  # Displaying the first 5 rows of X_test
print("\ny_test:")
print(y_test[:5])

# pipeline code for multiple cross validation comparison (MSE)
pipeline_lr = Pipeline([("scalar1", StandardScaler()),
                       ("lr", LinearRegression())])

pipeline_dt = Pipeline([("scalar2", StandardScaler()),
                       ("dt", DecisionTreeRegressor())])

pipeline_xgb = Pipeline([("scalar3", StandardScaler()),
                       ("xgb", XGBRegressor())])

pipeline_rf=Pipeline([("scalar4",StandardScaler()),
                     ("rf",RandomForestRegressor())])

pipeline_kn=Pipeline([("scalar5",StandardScaler()),
                     ("kn",KNeighborsRegressor())])

# List of pipelines
pipelines = [pipeline_lr, pipeline_dt, pipeline_xgb, pipeline_rf, pipeline_kn]

pipeline_dict = {0: "LinearRegression", 1: "DecisionTreeRegressor", 2: "XGBRegressor", 3: "RandomForestRegressor", 4: "KNeighborsRegressor"}

#Pipeline Fitting
for pipe in pipelines:
  pipe.fit(X_train, y_train)

# Print result for rmse
result_crossvalidation_rms = [] #Data stprage
for i, model in enumerate(pipelines):
    cv_score = cross_val_score(model, X_train,y_train,scoring="neg_root_mean_squared_error", cv=12)
    result_crossvalidation_rms.append(cv_score)
    print(f"{pipeline_dict[i]}: {-1 * cv_score.mean()} ")

"""## Random forest and XGBRegressor has the similiar rms, let's test them both

R2 Accuracy
"""

pred = pipeline_xgb.predict(X_test)
print("R^2:",metrics.r2_score(y_test, pred))

pred2 = pipeline_rf.predict(X_test)
print("R^2:",metrics.r2_score(y_test, pred2))

# Actual price (Train and test)

# RF fit
rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)

# XGBfit
xgb_model = XGBRegressor()
xgb_model.fit(X_train, y_train)

# RF Train test
rf_train_pred = rf_model.predict(X_train)
rf_test_pred = rf_model.predict(X_test)

# XGBoost Train Test
xgb_train_pred = xgb_model.predict(X_train)
xgb_test_pred = xgb_model.predict(X_test)

# Print random forest model
print("Random Forest Model:")
print("Train - Actual Prices:", y_train[:5])
print("Train - Predicted Prices:", rf_train_pred[:5])
print("Test - Actual Prices:", y_test[:5])
print("Test - Predicted Prices:", rf_test_pred[:5])
# Scatter plot for visualiztion ()

#Print XGBoost Model
print("XGBoost Model:")
print("Train - Actual Prices:", y_train[:5])
print("Train - Predicted Prices:", xgb_train_pred[:5])
print("Test - Actual Prices:", y_test[:5])
print("Test - Predicted Prices:", xgb_test_pred[:5])

"""#Prediction Visualization"""

# XGBoost Train
plt.figure(figsize=(8, 6))
plt.scatter(y_train, xgb_train_pred, color='blue', alpha=0.5)
plt.title('XGBoost Model: Actual vs. Predicted Prices (Training Set)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.grid(True)
plt.show()

#XGBoost Test
plt.figure(figsize=(8, 6))
plt.scatter(y_test, xgb_test_pred, color='red', alpha=0.5)
plt.title('XGBoost Model: Actual vs. Predicted Prices (Test Set)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(y_train, rf_train_pred, color='blue', alpha=0.5)
plt.title('RandomForest: Actual vs. Predicted Prices (Training Set)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(y_test, rf_test_pred, color='red', alpha=0.5)
plt.title('Random Forest Model: Actual vs. Predicted Prices (Test Set)')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.grid(True)
plt.show()

# SHAP Model evaluation
!pip install shap

import shap

sns.boxplot(data=df, x='carat', y='cut')
plt.title('Carat outlier')
plt.show

sns.boxplot(data=df, x='cut', y='price')
plt.title('cut-price outlier')
plt.show

